{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dcfKTkqtf_UV"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading and preparing data\n",
        "train_df = pd.read_csv('/content/train.tsv', delimiter='\\t', header=None)\n",
        "valid_df = pd.read_csv('/content/valid.tsv', delimiter='\\t', header=None)\n",
        "test_df = pd.read_csv('/content/test.tsv', delimiter='\\t', header=None)\n",
        "\n",
        "# Create a universal set of labels from all datasets\n",
        "all_labels = pd.concat([train_df[1], valid_df[1], test_df[1]])\n",
        "unique_labels = sorted(all_labels.unique())\n",
        "label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "\n",
        "# Convert labels using the universal mapping\n",
        "train_labels = to_categorical(train_df[1].map(label_mapping).values, num_classes=len(unique_labels))\n",
        "valid_labels = to_categorical(valid_df[1].map(label_mapping).values, num_classes=len(unique_labels))\n",
        "test_labels = to_categorical(test_df[1].map(label_mapping).values, num_classes=len(unique_labels))\n",
        "\n",
        "# Print to verify\n",
        "print(\"Label Mapping:\", label_mapping)\n",
        "print(\"Sample Encoded Train Labels:\", train_labels[:5])\n",
        "\n",
        "# Print shape of the labels arrays\n",
        "print(\"Train Labels Shape:\", train_labels.shape)\n",
        "\n",
        "# Check the first few labels to ensure they contain exactly one '1' per row\n",
        "for i in range(5):\n",
        "    print(f\"Label {i}: {train_labels[i]} - Sum: {np.sum(train_labels[i])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omYt8Kz6grfV",
        "outputId": "5c1b735e-bc91-4116-f2d6-31a02c780fdd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label Mapping: {'barely-true': 0, 'false': 1, 'half-true': 2, 'mostly-true': 3, 'pants-fire': 4, 'true': 5}\n",
            "Sample Encoded Train Labels: [[0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]]\n",
            "Train Labels Shape: (8919, 6)\n",
            "Label 0: [0. 1. 0. 0. 0. 0.] - Sum: 1.0\n",
            "Label 1: [0. 0. 1. 0. 0. 0.] - Sum: 1.0\n",
            "Label 2: [0. 0. 0. 1. 0. 0.] - Sum: 1.0\n",
            "Label 3: [0. 1. 0. 0. 0. 0.] - Sum: 1.0\n",
            "Label 4: [0. 0. 1. 0. 0. 0.] - Sum: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print unique labels from the dataset to ensure they are correct\n",
        "print(\"Unique labels in training data:\", train_df[0].unique())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AE4-LnVVp3os",
        "outputId": "8c4fed2b-539b-4b1f-c7e5-7bc926dccfa1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique labels in training data: ['2635.json' '10540.json' '324.json' ... '12269.json' '9658.json'\n",
            " '3951.json']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NY47XselqP7A",
        "outputId": "86c2e0e7-2088-4409-f901-1fae0347bfa2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           0            1                                                  2   \\\n",
            "0   2635.json        false  Says the Annies List political group supports ...   \n",
            "1  10540.json    half-true  When did the decline of coal start? It started...   \n",
            "2    324.json  mostly-true  Hillary Clinton agrees with John McCain \"by vo...   \n",
            "3   1123.json        false  Health care reform legislation is likely to ma...   \n",
            "4   9028.json    half-true  The economic turnaround started at the end of ...   \n",
            "\n",
            "                                   3               4                     5   \\\n",
            "0                            abortion    dwayne-bohac  State representative   \n",
            "1  energy,history,job-accomplishments  scott-surovell        State delegate   \n",
            "2                      foreign-policy    barack-obama             President   \n",
            "3                         health-care    blog-posting                   NaN   \n",
            "4                        economy,jobs   charlie-crist                   NaN   \n",
            "\n",
            "         6           7     8     9      10     11    12                   13  \n",
            "0     Texas  republican   0.0   1.0    0.0    0.0   0.0             a mailer  \n",
            "1  Virginia    democrat   0.0   0.0    1.0    1.0   0.0      a floor speech.  \n",
            "2  Illinois    democrat  70.0  71.0  160.0  163.0   9.0               Denver  \n",
            "3       NaN        none   7.0  19.0    3.0    5.0  44.0       a news release  \n",
            "4   Florida    democrat  15.0   9.0   20.0   19.0   2.0  an interview on CNN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_df[2])\n",
        "train_sequences = tokenizer.texts_to_sequences(train_df[2])\n",
        "train_padded = pad_sequences(train_sequences, maxlen=500)\n",
        "valid_sequences = tokenizer.texts_to_sequences(valid_df[2])\n",
        "valid_padded = pad_sequences(valid_sequences, maxlen=500)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_df[2])\n",
        "test_padded = pad_sequences(test_sequences, maxlen=500)\n",
        "\n",
        "# Build the model\n",
        "model = Sequential([\n",
        "    Embedding(10000, 64, input_length=500),\n",
        "    LSTM(64),\n",
        "    Dense(len(unique_labels), activation='softmax')  # Adjust the output layer\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# Training\n",
        "history = model.fit(train_padded, train_labels, epochs=10, validation_data=(valid_padded, valid_labels))\n",
        "\n",
        "# Evaluation\n",
        "test_loss, test_accuracy = model.evaluate(test_padded, test_labels)\n",
        "print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEzrGQ-5rnA0",
        "outputId": "b97bf816-6d42-4a19-9d4a-be7df7a2d401"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 500, 64)           640000    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 64)                33024     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 6)                 390       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 673414 (2.57 MB)\n",
            "Trainable params: 673414 (2.57 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "279/279 [==============================] - 93s 313ms/step - loss: 1.7464 - accuracy: 0.2223 - val_loss: 1.7178 - val_accuracy: 0.2422\n",
            "Epoch 2/10\n",
            "279/279 [==============================] - 88s 316ms/step - loss: 1.6035 - accuracy: 0.3306 - val_loss: 1.7639 - val_accuracy: 0.2352\n",
            "Epoch 3/10\n",
            "279/279 [==============================] - 89s 319ms/step - loss: 1.2903 - accuracy: 0.5048 - val_loss: 1.9401 - val_accuracy: 0.2352\n",
            "Epoch 4/10\n",
            "279/279 [==============================] - 88s 315ms/step - loss: 0.9192 - accuracy: 0.6632 - val_loss: 2.2692 - val_accuracy: 0.2111\n",
            "Epoch 5/10\n",
            "279/279 [==============================] - 91s 327ms/step - loss: 0.6378 - accuracy: 0.7782 - val_loss: 2.6329 - val_accuracy: 0.2220\n",
            "Epoch 6/10\n",
            "279/279 [==============================] - 92s 330ms/step - loss: 0.4335 - accuracy: 0.8527 - val_loss: 3.0544 - val_accuracy: 0.2227\n",
            "Epoch 7/10\n",
            "279/279 [==============================] - 90s 324ms/step - loss: 0.3065 - accuracy: 0.8999 - val_loss: 3.3827 - val_accuracy: 0.2227\n",
            "Epoch 8/10\n",
            "279/279 [==============================] - 96s 344ms/step - loss: 0.2094 - accuracy: 0.9346 - val_loss: 3.7624 - val_accuracy: 0.2196\n",
            "Epoch 9/10\n",
            "279/279 [==============================] - 91s 325ms/step - loss: 0.1554 - accuracy: 0.9535 - val_loss: 4.1943 - val_accuracy: 0.2079\n",
            "Epoch 10/10\n",
            "279/279 [==============================] - 90s 324ms/step - loss: 0.1163 - accuracy: 0.9650 - val_loss: 4.6429 - val_accuracy: 0.2157\n",
            "40/40 [==============================] - 3s 77ms/step - loss: 4.5578 - accuracy: 0.2218\n",
            "Test Loss: 4.557751655578613, Test Accuracy: 0.22178374230861664\n"
          ]
        }
      ]
    }
  ]
}